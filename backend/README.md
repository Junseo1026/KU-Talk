KKU CS Notice Crawler

This small crawler fetches notices from the KKU CS department page and saves
raw HTML and parsed JSON into `backend/data/`.

How to use
- Create a Python virtualenv and install requirements: `pip install -r requirements.txt`
- Edit `config.py` if you need to change the target URL or CSS selectors.
- Run: `python run_crawler.py --max-pages 3`
 - Run: `python run_crawler.py --max-pages 3`
   - To crawl all pages until the oldest: `python run_crawler.py --max-pages 0` (or a negative number)

Output
- `data/raw/` contains downloaded HTML files per post.
- `data/parsed/` contains extracted JSON metadata and content per post.
- `data/index.json` contains a list of known posts and their metadata.

API
- A small FastAPI app is provided at `backend/api.py` to serve collected
  posts for a ChatBot. To run locally:
  - `pip install -r requirements.txt`
  - `uvicorn api:app --reload --host 127.0.0.1 --port 8000` (run from `backend/`)
- Endpoints: `GET /posts`, `GET /posts/{id}`, `GET /search?q=...`

Scheduler (daily crawl)
- A scheduler script is provided to run the crawler daily at 08:00 KST.
- It uses APScheduler. To run it in background:
  - `set -o allexport; source backend/.env; set +o allexport`
  - `cd backend`
  - `nohup python scheduler.py > scheduler.log 2>&1 &`
- The scheduler currently runs the crawler for the first 3 pages each day
  and will download new posts and run OCR on images.

Cron alternative
- If you prefer cron instead of the Python scheduler, add a cron entry such as:
  - `0 8 * * * cd /full/path/to/chat_bot/backend && /bin/bash -lc "set -o allexport; source .env; set +o allexport; python run_crawler.py --max-pages 3 >> cron_crawl.log 2>&1"`

Proxy / Corporate network
- If your network requires an HTTP/HTTPS proxy, add proxy settings to `backend/.env`:
  - `HTTP_PROXY=http://proxy.host:3128`
  - `HTTPS_PROXY=http://proxy.host:3128`
- The crawler will detect these environment variables and route requests through the proxy.
- After editing `backend/.env`, load it before running the crawler:
  - `set -o allexport; source backend/.env; set +o allexport`

LLM (optional)
- If you want Chat responses to be generated by an LLM (OpenAI), set
  the environment variable `OPENAI_API_KEY` with your API key before
  starting the server. When present, the `/chat` endpoint will call the
  OpenAI Chat Completions API to produce a concise, natural answer.
- Example (Linux/mac):
  - `export OPENAI_API_KEY=sk-...`
  - `uvicorn api:app --reload --host 127.0.0.1 --port 8000`

Security note: keep your API key secret and do not commit it to source control.

Using a .env file
- A convenient way to store your `OPENAI_API_KEY` locally is to put it in
  `backend/.env`. The repository already contains `backend/.env` with a
  placeholder value. Edit the file and replace `여기에 입력하세요` with your
  actual API key.

  Example (edit in place):
    - `sed -i 's/여기에 입력하세요/sk-REPLACE_WITH_KEY/' backend/.env`

  Load the variables into your shell before starting the server:
    - `set -o allexport; source backend/.env; set +o allexport`

  Note: `backend/.gitignore` contains `.env` to help avoid committing the
  key. Still, double-check before committing.
